{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820656f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6caf173c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Folder structure\n",
    "\n",
    "The following folder structure will be used in this project\n",
    "\n",
    "VoiceSimilarityAnalysis-code\n",
    "â”œâ”€â”€ data\n",
    "â”‚   â”œâ”€â”€ download              # Contains downloaded zip files (e.g. ABI-1_Corpus.zip)\n",
    "â”‚   â”œâ”€â”€ raw/                   # Unzipped original dataset (14 accent folders)\n",
    "â”‚   â””â”€â”€ cleansed/              # Contains only the filtered \"shortpassage\" .wav files\n",
    "â”‚\n",
    "â”œâ”€â”€ reports/                  # Drafts and final version of the report\n",
    "â”‚\n",
    "â”œâ”€â”€ results/                  # Output files: embeddings, similarity scores, matrices, plots\n",
    "â”‚\n",
    "â”œâ”€â”€ appendix/                 # Generative AI chat logs for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769ac352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports go here\n",
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a72ea",
   "metadata": {},
   "source": [
    "## Step 1 - download the dataset\n",
    "\n",
    "In an effort to totally automate the process, the dataset will be downloaded in a raw-data folder using the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bbad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=18FWBn4B6gQifOtf1C9JCQv4Lrs8C1uvu\n",
      "From (redirected): https://drive.google.com/uc?id=18FWBn4B6gQifOtf1C9JCQv4Lrs8C1uvu&confirm=t&uuid=e299ebfa-0e41-4c1f-8280-7b9ba24e9779\n",
      "To: /Users/carmelgafa/Documents/my-work/ari5121-project/VoiceSimilarityAnalysis-code/data/download/ABI-1_Corpus/ABI-1_Corpus.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.82G/2.82G [04:27<00:00, 10.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data downloaded...\n"
     ]
    }
   ],
   "source": [
    "# 7 minutes on Costa wifi -- execute and have a coffee\n",
    "\n",
    "# gdrive id\n",
    "file_id = \"18FWBn4B6gQifOtf1C9JCQv4Lrs8C1uvu\"\n",
    "\n",
    "# download the file\n",
    "download_folder = os.path.join(\"data\", \"download\", \"ABI-1_Corpus\")\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "download_path = os.path.join(download_folder, \"ABI-1_Corpus.zip\")\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", download_path, quiet=False)\n",
    "\n",
    "# open the zip\n",
    "raw_folder = os.path.join(\"data\", \"raw\", \"ABI-1_Corpus\")\n",
    "if not os.path.exists(raw_folder):\n",
    "    os.makedirs(raw_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(raw_folder)\n",
    "\n",
    "print(\"Raw data downloaded...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7081537",
   "metadata": {},
   "source": [
    "## Step 2 - Cleanse the dataset\n",
    "\n",
    "We will only keep the \"shortpassage*.wav\" files for each accent in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235d557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansing completed...\n"
     ]
    }
   ],
   "source": [
    "raw_data_folder = \"data/raw/ABI-1_Corpus/ABI-1 Corpus/accents\"\n",
    "cleansed_dir = \"data/cleansed\"\n",
    "\n",
    "\n",
    "# list accent folders removing the annoying folders\n",
    "accents = [accent_folder for accent_folder in os.listdir(raw_data_folder) if not accent_folder.startswith(\".\")]\n",
    "# go through all accents\n",
    "for accent in accents:\n",
    "    accent_path = os.path.join(raw_data_folder, accent)\n",
    "    # list all genders in each accent\n",
    "    genders = [gender_folder for gender_folder in os.listdir(accent_path) if not gender_folder.startswith(\".\")]\n",
    "    # go through all genders in each accent\n",
    "    for gender in genders:  \n",
    "        gender_folder = os.path.join(accent_path, gender)\n",
    "        # go througgh each speaker in each gender\n",
    "        # lsit all speakers\n",
    "        speakers = [speaker for speaker in os.listdir(gender_folder) if not speaker.startswith(\".\")]\n",
    "        for speaker in speakers:\n",
    "            speaker_path = os.path.join(gender_folder, speaker)\n",
    "\n",
    "            # store resulting  data in cleaned/accent/gender/speaker\n",
    "            dest_path = os.path.join(cleansed_dir, accent, gender, speaker)\n",
    "            os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "            # copy only filenames that  are shortpassage*.wav\n",
    "            # go throough all files        \n",
    "            for filename in os.listdir(speaker_path):\n",
    "                if re.fullmatch(r\"shortpassage.*\\.wav\", filename):\n",
    "\n",
    "                    src_file = os.path.join(speaker_path, filename)\n",
    "                    dst_file = os.path.join(dest_path, filename)\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "\n",
    "print(\"Cleansing completed...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Removing existing model folder: model\n",
      "Model downloaded...\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, AutoModel\n",
    "\n",
    "\n",
    "# model folder at microsoft and corresponding local folder\n",
    "model_name = \"microsoft/wavlm-base-plus-sv\"\n",
    "local_dir = \"model\"\n",
    "\n",
    "# empty the model folder\n",
    "if os.path.exists(local_dir):\n",
    "    print(f\"ðŸ§¹ Removing existing model folder: {local_dir}\")\n",
    "    shutil.rmtree(local_dir)\n",
    "\n",
    "\n",
    "\n",
    "# downlad the model\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name, cache_dir=local_dir)\n",
    "model = AutoModel.from_pretrained(model_name, cache_dir=local_dir)\n",
    "\n",
    "print(f\"Model downloaded...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a246af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Just testing the code on huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c16fa10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri data/cleansed/BRM/male/spk01/shortpassage_001.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load a .wav file\u001b[39;00m\n\u001b[32m     12\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mdata/cleansed/BRM/male/spk01/shortpassage_001.wav\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# <- Change this as needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m waveform, sample_rate = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Resample to 16kHz if needed\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_rate != \u001b[32m16000\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-work/ari5121-project/.venv/lib/python3.11/site-packages/torchaudio/_backend/utils.py:204\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    119\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    120\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    127\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     backend = \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.load(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/my-work/ari5121-project/.venv/lib/python3.11/site-packages/torchaudio/_backend/utils.py:116\u001b[39m, in \u001b[36mget_load_func.<locals>.dispatcher\u001b[39m\u001b[34m(uri, format, backend_name)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend.can_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Couldn't find appropriate backend to handle uri data/cleansed/BRM/male/spk01/shortpassage_001.wav and format None."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor, AutoModel\n",
    "\n",
    "# Load model and feature extractor\n",
    "model_name = \"microsoft/wavlm-base-plus-sv\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()  # Inference mode\n",
    "\n",
    "# Load a .wav file\n",
    "file_path = \"data/cleansed/BRM/male/spk01/shortpassage_001.wav\"  # <- Change this as needed\n",
    "waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "# Resample to 16kHz if needed\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Convert stereo to mono if needed\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# Extract input values\n",
    "inputs = feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through model to get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state  # (batch_size, time_steps, feature_dim)\n",
    "\n",
    "# Aggregate embeddings (e.g., mean pooling across time)\n",
    "embedding = last_hidden_state.mean(dim=1).squeeze()  # shape: (feature_dim,)\n",
    "\n",
    "print(f\"âœ… Extracted embedding shape: {embedding.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
